{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening(X):\n",
    "    (a,b,c,d) = X.shape\n",
    "    X = np.reshape(X,(a,b*c*d))\n",
    "    return X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_flattening(X,a,b,c,d):\n",
    "    X = np.reshape(X,(a,b,c,d))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA,Z):\n",
    "    Z[Z>0] = 1\n",
    "    Z[Z<0] = 0\n",
    "    return np.multiply(dA,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    A = np.exp(z)\n",
    "    B = np.sum(np.exp(z))\n",
    "    return A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant', constant_values = (0,0))\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    average = dz/(n_H*n_W)\n",
    "    \n",
    "    a = np.full((n_H,n_W), average)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    \n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    Z = Z+b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfun(Y,Y_hat):\n",
    "    (m,n) = Y.shape\n",
    "    A = np.multiply(Y,np.log(Y_hat))\n",
    "    cost1 = np.sum(A, axis = 0)\n",
    "    cost = (-1/m)*np.sum(cost1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnormalization(z):\n",
    "    mean = np.mean(z)\n",
    "    std = np.std(z)\n",
    "    z = np.divide((z-mean),std)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "\n",
    "  N, D = x.shape\n",
    "\n",
    "  #step1: calculate mean\n",
    "  mu = 1./N * np.sum(x, axis = 0)\n",
    "\n",
    "  #step2: subtract mean vector of every trainings example\n",
    "  xmu = x - mu\n",
    "\n",
    "  #step3: following the lower branch - calculation denominator\n",
    "  sq = xmu ** 2\n",
    "\n",
    "  #step4: calculate variance\n",
    "  var = 1./N * np.sum(sq, axis = 0)\n",
    "\n",
    "  #step5: add eps for numerical stability, then sqrt\n",
    "  sqrtvar = np.sqrt(var + eps)\n",
    "\n",
    "  #step6: invert sqrtwar\n",
    "  ivar = 1./sqrtvar\n",
    "\n",
    "  #step7: execute normalization\n",
    "  xhat = xmu * ivar\n",
    "\n",
    "  #step8: Nor the two transformation steps\n",
    "  gammax = gamma * xhat\n",
    "\n",
    "  #step9\n",
    "  out = gammax + beta\n",
    "\n",
    "  #store intermediate\n",
    "  cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)\n",
    "\n",
    "  return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "\n",
    "  #unfold the variables stored in cache\n",
    "  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache\n",
    "\n",
    "  #get the dimensions of the input/output\n",
    "  N,D = dout.shape\n",
    "\n",
    "  #step9\n",
    "  dbeta = np.sum(dout, axis=0)\n",
    "  dgammax = dout #not necessary, but more understandable\n",
    "\n",
    "  #step8\n",
    "  dgamma = np.sum(dgammax*xhat, axis=0)\n",
    "  dxhat = dgammax * gamma\n",
    "\n",
    "  #step7\n",
    "  divar = np.sum(dxhat*xmu, axis=0)\n",
    "  dxmu1 = dxhat * ivar\n",
    "\n",
    "  #step6\n",
    "  dsqrtvar = -1. /(sqrtvar**2) * divar\n",
    "\n",
    "  #step5\n",
    "  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar\n",
    "\n",
    "  #step4\n",
    "  dsq = 1. /N * np.ones((N,D)) * dvar\n",
    "\n",
    "  #step3\n",
    "  dxmu2 = 2 * xmu * dsq\n",
    "\n",
    "  #step2\n",
    "  dx1 = (dxmu1 + dxmu2)\n",
    "  dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n",
    "\n",
    "  #step1\n",
    "  dx2 = 1. /N * np.ones((N,D)) * dmu\n",
    "\n",
    "  #step0\n",
    "  dx = dx1 + dx2\n",
    "\n",
    "  return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "      \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    n_H = int((n_H_prev-f+2*pad)/stride) + 1 \n",
    "    n_W = int((n_W_prev-f+2*pad)/stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]                               \n",
    "        for h in range(n_H):                           \n",
    "            for w in range(n_W):                       \n",
    "                for c in range(n_C):                   \n",
    "                    \n",
    "                    vert_start = h *stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "       \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward convolution \n",
    "def conv_backward(dZ, cache):\n",
    "    \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       \n",
    "        \n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = A_prev_pad[i,vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "    \n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         \n",
    "        for h in range(n_H):                     \n",
    "            for w in range(n_W):                 \n",
    "                for c in range (n_C):            \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "        \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward pooling \n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros((A_prev.shape))\n",
    "    \n",
    "    for i in range(m):                       \n",
    "        \n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        \n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "        \n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        da = dA[i,h,w,c]\n",
    "                \n",
    "                        shape = (f,f)\n",
    "                        \n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "mndata = MNIST('/home/shabaz/Documents/python-mnist/')\n",
    "X_train, y_train = mndata.load_training() \n",
    "\n",
    "X_train = np.array(X_train)  #Data Scaling \n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "\n",
    "X = X_train[:500,:,:,:]\n",
    "y_train = y_train.reshape(X_train.shape[0],1)\n",
    "Y = y_train[:500,:]\n",
    "Y = convert_to_one_hot(Y, 10).T\n",
    "(n,l,m,o) = X.shape\n",
    "\n",
    "cost = []\n",
    "\n",
    "##################### Hyperparameter declaration ############\n",
    "# for Forward convolution\n",
    "W = np.random.randn(3,3,1,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "\n",
    "alpha = 0.001   # for Convolotion part\n",
    "alpha1 = 0.001  # for first layer of neural network  (RelU)\n",
    "alpha2 = 0.001  # for second layer of neural network (Softmax)\n",
    "gamma1,gamma2 = np.ones((1,10)),np.ones((1,10))\n",
    "beta1,beta2 = np.zeros((1,10)),np.zeros((1,10))\n",
    "eps = 0.00001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.517193191416238, 8.51610121572464, 8.515065486851206, 8.514086134192862, 8.513163287176768, 8.512297075241882, 8.511487627819363, 8.510735074311878, 8.510039544071734, 8.509401166377948, 8.50882007041208]\n",
      "[8.517193191416238, 8.51610121572464, 8.515065486851206, 8.514086134192862, 8.513163287176768, 8.512297075241882, 8.511487627819363, 8.510735074311878, 8.510039544071734, 8.509401166377948, 8.50882007041208, 8.50829638523301]\n",
      "[8.517193191416238, 8.51610121572464, 8.515065486851206, 8.514086134192862, 8.513163287176768, 8.512297075241882, 8.511487627819363, 8.510735074311878, 8.510039544071734, 8.509401166377948, 8.50882007041208, 8.50829638523301, 8.507830239750497]\n",
      "[8.517193191416238, 8.51610121572464, 8.515065486851206, 8.514086134192862, 8.513163287176768, 8.512297075241882, 8.511487627819363, 8.510735074311878, 8.510039544071734, 8.509401166377948, 8.50882007041208, 8.50829638523301, 8.507830239750497, 8.507421762697671]\n",
      "[8.517193191416238, 8.51610121572464, 8.515065486851206, 8.514086134192862, 8.513163287176768, 8.512297075241882, 8.511487627819363, 8.510735074311878, 8.510039544071734, 8.509401166377948, 8.50882007041208, 8.50829638523301, 8.507830239750497, 8.507421762697671, 8.507071082602286]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # for Forward convolution\n",
    "    hparameters = {\"pad\" : 1,\"stride\":1}\n",
    "    Z, cache_conv = conv_forward(X, W, b, hparameters)\n",
    "\n",
    "    # Implementing RelU \n",
    "    R1 = relu(Z)\n",
    "\n",
    "    # Pooling \n",
    "    hparameters = {\"stride\" : 2, \"f\": 2}\n",
    "    A, cache = pool_forward(R1, hparameters,mode = \"max\")\n",
    "    (a,l,m,o) = A.shape\n",
    "\n",
    "    ######### Hyperparameter for forward and backward ##########\n",
    "    ############### Weight for neural layers ###################\n",
    "    W1 = np.random.randn(l*m*o,10)*0.01\n",
    "    W2 = np.random.randn(10,10)*0.01\n",
    "\n",
    "    # bias for neural layers\n",
    "    b1 = np.random.randn(1,1)*0.01\n",
    "    b2 = np.random.randn(1,1)*0.01\n",
    "\n",
    "    # Flattening\n",
    "    S = flattening(A)\n",
    "    \n",
    "    ###################### Forward Prop #######################\n",
    "    ###################### Layer 1 ############################\n",
    "    Z1 = np.dot(S.T,W1)+b1\n",
    "    ############# Batch Normalization of first FC #############\n",
    "    #S = batchnormalization(S)\n",
    "    Z1, cache1 = batchnorm_forward(Z1, gamma1, beta1, eps)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    ###################### Layer 2 ############################\n",
    "    Z2 = np.dot(A1,W2)+b2\n",
    "    ############# Batch Normalization of Second FC ############\n",
    "    Z2, cache2 = batchnorm_forward(Z2, gamma2, beta2, eps)\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    ##################### Backward Prop #######################\n",
    "    ################### Layer 2 ###############################\n",
    "    dZ2 = dZ2 = A2 - Y  \n",
    "    \n",
    "    ############## Backward batch normalization ###############\n",
    "    dZ2,dgamma2,dbeta2 = batchnorm_backward(dZ2, cache2)\n",
    "    gamma2 = gamma2 - alpha2*dgamma2\n",
    "    beta2 = beta2 - alpha2*dbeta2\n",
    "    \n",
    "    dW2 = (1/n)*np.dot(A1.T,dZ2)\n",
    "    W2 = W2 - alpha2*dW2\n",
    "    db2 = (1/n)*np.sum(dZ2)\n",
    "    b2 = b2 - alpha2*db2\n",
    "    \n",
    "    ###################### Layer 1 ############################\n",
    "    dA1 = np.dot(dZ2,W2)\n",
    "    dZ1_temp = np.dot(dZ2,W2)\n",
    "    dZ1 = relu_back(dZ1_temp,Z1)\n",
    "    \n",
    "    ############## Backward batch normalization ###############\n",
    "    dZ1,dgamma1,dbeta1 = batchnorm_backward(dZ1, cache1)\n",
    "    gamma1 = gamma1 - alpha1*dgamma1\n",
    "    beta1 = beta1 - alpha1*dbeta1\n",
    "    \n",
    "    dW1 = (1/n)*np.dot(S,dZ1)\n",
    "    W1 = W1 - alpha1*dW1\n",
    "    db1 = (1/n)*np.sum(dZ1)\n",
    "    b1 = b1 - alpha1*db1\n",
    "\n",
    "    ##################### Cost calculation ####################\n",
    "    cost.append(costfun(Y,A2))\n",
    "    print(cost)\n",
    "    \n",
    "    ##################### Backward pooling ####################\n",
    "    dS_prev = np.dot(W1,dZ1.T)\n",
    "    dS = inv_flattening(dS_prev,a,l,m,o)\n",
    "    dA_prev = pool_backward(dS, cache, mode = \"max\")\n",
    "\n",
    "    #################### Backward RelU ########################\n",
    "    dR1 = relu_back(dA_prev,R1)\n",
    "\n",
    "    #################### Backwar convolution ##################\n",
    "    dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "    #################### Parameters Update ####################\n",
    "    W = W - alpha*dW\n",
    "    b = b - alpha*db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the model is 0.132\n"
     ]
    }
   ],
   "source": [
    "######################## Accuracy Calculation ########################\n",
    "A2[np.reshape(np.amax(A2,axis=1),(n,1))==A2] = 1\n",
    "A2[np.reshape(np.amax(A2,axis=1),(n,1))!=A2] = 0\n",
    "\n",
    "acc_mat = np.multiply(A2,Y)\n",
    "acc = np.sum(acc_mat)/n\n",
    "\n",
    "print(\"Train accuracy of the model is\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.517193191416242,\n",
       " 8.516101215724646,\n",
       " 8.515065486851206,\n",
       " 8.51408613419286,\n",
       " 8.51316328717677,\n",
       " 8.512297075241882,\n",
       " 8.511487627819372,\n",
       " 8.510735074311878,\n",
       " 8.510039544071732,\n",
       " 8.509401166377947,\n",
       " 8.508820070412082,\n",
       " 8.508296385233,\n",
       " 8.5078302397505,\n",
       " 8.507421762697668,\n",
       " 8.507071082602288,\n",
       " 8.50677832775693,\n",
       " 8.506543626187995,\n",
       " 8.506367105623616,\n",
       " 8.506248893460402,\n",
       " 8.506189116729066]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
