{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattening(X):\n",
    "    (a,b,c,d) = X.shape\n",
    "    X = np.reshape(X,(a,b*c*d))\n",
    "    return X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_flattening(X,a,b,c,d):\n",
    "    X = np.reshape(X,(a,b,c,d))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(dA,Z):\n",
    "    Z[Z>0] = 1\n",
    "    Z[Z<0] = 0\n",
    "    return np.multiply(dA,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    A = np.exp(z)\n",
    "    B = np.sum(np.exp(z))\n",
    "    return A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant', constant_values = (0,0))\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    average = dz/(n_H*n_W)\n",
    "    \n",
    "    a = np.full((n_H,n_W), average)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    \n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    Z = Z+b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costfun(Y,Y_hat):\n",
    "    (m,n) = Y.shape\n",
    "    A = np.multiply(Y,np.log(Y_hat))\n",
    "    cost1 = np.sum(A, axis = 0)\n",
    "    cost = (-1/m)*np.sum(cost1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnormalization(z):\n",
    "    mean = np.mean(z)\n",
    "    std = np.std_dev(z)\n",
    "    z = np.divide((z-mean),std)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "      \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    n_H = int((n_H_prev-f+2*pad)/stride) + 1 \n",
    "    n_W = int((n_W_prev-f+2*pad)/stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]                               \n",
    "        for h in range(n_H):                           \n",
    "            for w in range(n_W):                       \n",
    "                for c in range(n_C):                   \n",
    "                    \n",
    "                    vert_start = h *stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])\n",
    "                                        \n",
    "       \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward convolution \n",
    "def conv_backward(dZ, cache):\n",
    "    \n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       \n",
    "        \n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = A_prev_pad[i,vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "    \n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         \n",
    "        for h in range(n_H):                     \n",
    "            for w in range(n_W):                 \n",
    "                for c in range (n_C):            \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "        \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward pooling \n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros((A_prev.shape))\n",
    "    \n",
    "    for i in range(m):                       \n",
    "        \n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   \n",
    "            for w in range(n_W):               \n",
    "                for c in range(n_C):           \n",
    "                    \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        \n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "        \n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        da = dA[i,h,w,c]\n",
    "                \n",
    "                        shape = (f,f)\n",
    "                        \n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "mndata = MNIST('/home/shabaz/Documents/python-mnist/')\n",
    "X_train, y_train = mndata.load_training() \n",
    "\n",
    "X_train = np.array(X_train)  #Data Scaling \n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "\n",
    "X = X_train[:500,:,:,:]\n",
    "y_train = y_train.reshape(X_train.shape[0],1)\n",
    "Y = y_train[:500,:]\n",
    "Y = convert_to_one_hot(Y, 10).T\n",
    "(n,l,m,o) = X.shape\n",
    "\n",
    "cost = []\n",
    "\n",
    "##################### Hyperparameter declaration ############\n",
    "# for Forward convolution\n",
    "W = np.random.randn(3,3,1,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "\n",
    "alpha = 0.001   # for Convolotion part\n",
    "alpha1 = 0.001  # for first layer of neural network  (RelU)\n",
    "alpha2 = 0.001  # for second layer of neural network (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.51664664546852, 8.517899848430664, 8.518427967178082, 8.517070559057213, 8.516985984461927, 8.51798951597624, 8.518502207241127, 8.51660700077845, 8.517707706300577, 8.517658234270971, 8.516197042821839]\n",
      "[8.51664664546852, 8.517899848430664, 8.518427967178082, 8.517070559057213, 8.516985984461927, 8.51798951597624, 8.518502207241127, 8.51660700077845, 8.517707706300577, 8.517658234270971, 8.516197042821839, 8.517246015856951]\n",
      "[8.51664664546852, 8.517899848430664, 8.518427967178082, 8.517070559057213, 8.516985984461927, 8.51798951597624, 8.518502207241127, 8.51660700077845, 8.517707706300577, 8.517658234270971, 8.516197042821839, 8.517246015856951, 8.516314543196247]\n",
      "[8.51664664546852, 8.517899848430664, 8.518427967178082, 8.517070559057213, 8.516985984461927, 8.51798951597624, 8.518502207241127, 8.51660700077845, 8.517707706300577, 8.517658234270971, 8.516197042821839, 8.517246015856951, 8.516314543196247, 8.518147743079094]\n",
      "[8.51664664546852, 8.517899848430664, 8.518427967178082, 8.517070559057213, 8.516985984461927, 8.51798951597624, 8.518502207241127, 8.51660700077845, 8.517707706300577, 8.517658234270971, 8.516197042821839, 8.517246015856951, 8.516314543196247, 8.518147743079094, 8.516327803167608]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    # for Forward convolution\n",
    "    hparameters = {\"pad\" : 1,\"stride\": 1}\n",
    "    Z, cache_conv = conv_forward(X, W, b, hparameters)\n",
    "\n",
    "    # Implementing RelU \n",
    "    R1 = relu(Z)\n",
    "\n",
    "    # Pooling \n",
    "    hparameters = {\"stride\" : 2, \"f\": 2}\n",
    "    A, cache = pool_forward(R1, hparameters,mode = \"max\")\n",
    "    (a,l,m,o) = A.shape\n",
    "\n",
    "    ########## Hyperparameter for forward and backward ########\n",
    "    # Weight for neural layers\n",
    "    W1 = np.random.randn(l*m*o,10)*0.01\n",
    "    W2 = np.random.randn(10,10)*0.01\n",
    "\n",
    "    # bias for neural layers\n",
    "    b1 = np.random.randn(1,1)*0.01\n",
    "    b2 = np.random.randn(1,1)*0.01\n",
    "\n",
    "    # Flattening\n",
    "    S = flattening(A)\n",
    "    \n",
    "    ###################### Forward Prop #######################\n",
    "    ###################### Layer 1 ############################\n",
    "    Z1 = np.dot(S.T,W1)+b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    ###################### Layer 2 ############################\n",
    "    Z2 = np.dot(A1,W2)+b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    ##############1####### Backward Prop #######################\n",
    "    ################### Layer 2 ###############################\n",
    "    dZ2 = dZ2 = A2 - Y  \n",
    "    dW2 = (1/n)*np.dot(A1.T,dZ2)\n",
    "    W2 = W2 - alpha2*dW2\n",
    "    db2 = (1/n)*np.sum(dZ2)\n",
    "    b2 = b2 - alpha2*db2\n",
    "\n",
    "    ###################### Layer 1 ############################\n",
    "    dA1 = np.dot(dZ2,W2)\n",
    "    dZ1_temp = np.dot(dZ2,W2)\n",
    "    dZ1 = relu_back(dZ1_temp,Z1)\n",
    "    dW1 = (1/n)*np.dot(S,dZ1)\n",
    "    W1 = W1 - alpha1*dW1\n",
    "    db1 = (1/n)*np.sum(dZ1)\n",
    "    b1 = b1 - alpha1*db1\n",
    "\n",
    "    ##################### Cost calculation ####################\n",
    "    cost.append(costfun(Y,A2))\n",
    "    print(cost)\n",
    "    \n",
    "    ##################### Backward pooling ####################\n",
    "    dS_prev = np.dot(W1,dZ1.T)\n",
    "    dS = inv_flattening(dS_prev,a,l,m,o)\n",
    "    dA_prev = pool_backward(dS, cache, mode = \"max\")\n",
    "\n",
    "    #################### Backward RelU ########################\n",
    "    dR1 = relu_back(dA_prev,R1)\n",
    "\n",
    "    #################### Backwar convolution ##################\n",
    "    dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "    #################### Parameters Update ####################\n",
    "    W = W - alpha*dW\n",
    "    b = b - alpha*db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the model is 0.1\n"
     ]
    }
   ],
   "source": [
    "######################## Accuracy Calculation ########################\n",
    "A2[np.reshape(np.amax(A2,axis=1),(n,1))==A2] = 1\n",
    "A2[np.reshape(np.amax(A2,axis=1),(n,1))!=A2] = 0\n",
    "\n",
    "acc_mat = np.multiply(A2,Y)\n",
    "acc = np.sum(acc_mat)/n\n",
    "\n",
    "print(\"Train accuracy of the model is\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
